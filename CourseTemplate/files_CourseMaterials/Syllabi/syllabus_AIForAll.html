<h1> Talk 1 (Jan 27, 2026): Generative AI and LLM on Your Laptop and on HPC Cluster: hat is the Difference </h1>
<h2> Dr. Orion Lawlor </h2>

<p>
  <h3 >Abstract </h3>
  At our AI for All workshop this Tuesday from 10am-noon, we'll explore how you can run Large Language Models (LLMs) on your own local hardware using llamafile, ollama, and llama.cpp.  These models work best on a machine with a recent GPU or an Apple silicon Mac, but small models will run slowly on anything, and a local model gives much better privacy, predictability and technical control than is available with a cloud-hosted model.  This provides a clearer view of the underlying mechanics, limitations, and promise of this technology.  We will also demonstrate how we can run LLMs on the new GPUs on UAF's Chinook supercomputer, allowing us to use large models with the same local control.

  We will be installing several programs and discussing the internals of the models, and command line experience will be beneficial, but no programming is required.
</p>

<p>
  <h3> Materials shared by the speaker </h3>
  <b> Slide: </b>  <a href=' https://docs.google.com/presentation/d/1SrcGs9bwII5F3lhE_FWHmBwrxJCiJ6gZXCIvaj2rSdM/edit?slide=id.p#slide=id.p' target='_blank'>Link to Google slide</a>
  <br>
  <b> More Documents: </b> <a href='https://docs.google.com/document/d/1opM_sGndXlB5OVG8H9REi15bbyrnfQsRaEbhN2J8aPY/edit?tab=t.0' target='_blank'> Link to Google Doc </a> (anyone can comment)
</p>
