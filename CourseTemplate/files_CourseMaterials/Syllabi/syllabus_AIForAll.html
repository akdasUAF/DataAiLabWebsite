<h1> Talk 1 (Jan 27, 2026): Generative AI and LLM on Your Laptop and on HPC Cluster: What is the Difference </h1>
<h2> Speaker:  Orion Lawlor </h2>

<p>
  <h3 >Abstract </h3>
  At our AI for All workshop this Tuesday from 10am-noon, we'll explore how you can run Large Language Models (LLMs) on your own local hardware using llamafile, ollama, and llama.cpp.  These models work best on a machine with a recent GPU or an Apple silicon Mac, but small models will run slowly on anything, and a local model gives much better privacy, predictability and technical control than is available with a cloud-hosted model.  This provides a clearer view of the underlying mechanics, limitations, and promise of this technology.  We will also demonstrate how we can run LLMs on the new GPUs on UAF's Chinook supercomputer, allowing us to use large models with the same local control.

  We will be installing several programs and discussing the internals of the models, and command line experience will be beneficial, but no programming is required.
</p>

<p>
  <h3> Materials shared by the speaker </h3>
  <b> Slide: </b>  <a href=' https://docs.google.com/presentation/d/1SrcGs9bwII5F3lhE_FWHmBwrxJCiJ6gZXCIvaj2rSdM/edit?slide=id.p#slide=id.p' target='_blank'>Link to Google slide</a>
  <br>
  <b> How to run an LLM model Locally: </b> <a href='https://docs.google.com/document/d/1opM_sGndXlB5OVG8H9REi15bbyrnfQsRaEbhN2J8aPY/edit?tab=t.0' target='_blank'> Link to Google Doc </a> (anyone can comment)
  <br>
  <b> A Chat Agent Hosted at UAF Chinook HPC: </b><a href='https://aurora.cs.uaf.edu/llm/gemma3/'> Link to a chat-agent </a> <br>(This instance is only for research, develoment, and testing. Do Not provide any confidential, or proprietery, or personal information)
</p>
<hr>
<h1> Talk 2 (Feb 10, 2026): Linux Foundation: Beginnerâ€™s intro to command line </h1>
<h2> Speaker: Kevin Galloway </h2>

<p>
  <h3 >Abstract </h3>

</p>

<p>
  <h3> Materials shared by the speaker </h3>
  Will be available here after the talk
</p>
